{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860fe2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library and some pre-installed modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import glob\n",
    "import threading\n",
    "import csv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from shapely.geometry import box, mapping\n",
    "from scipy.optimize import curve_fit\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd310426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the root directory of the project as the working directory\n",
    "os.chdir('..')\n",
    "# Import the custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36379e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Usuario\\\\Dev\\\\PhD_Thesis_Step3_OSM_Toponyms'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current working directory\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e705ee5",
   "metadata": {},
   "source": [
    "### Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9d9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom module\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5adb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils' from 'c:\\\\Users\\\\Usuario\\\\Dev\\\\PhD_Thesis_Step3_OSM_Toponyms\\\\src\\\\utils.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the utils module to ensure any changes are reflected\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a032fea",
   "metadata": {},
   "source": [
    "### Homogeneous Grid Cells\n",
    " - Statistical Grid (cell size of 200 x 200m) produced by Instituto Brasileiro de Geografia e Estatística (Brazilian Institute of Geography and Statistics)\n",
    "\n",
    "  - https://geoftp.ibge.gov.br/recortes_para_fins_estatisticos/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fd756",
   "metadata": {},
   "source": [
    "#### Import Homogeneous Grid Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7c078",
   "metadata": {},
   "source": [
    "##### Import the grid with the aggregated data extracted from OSM via the OHSOME API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statistics grid in GeoJSON format\n",
    "grid = None\n",
    "\n",
    "input_path = 'data\\input_code1'\n",
    "\n",
    "# Function for selecting and loading the GeoJSON file\n",
    "def select_file(change):\n",
    "    global grid\n",
    "    selected_file = change['new']\n",
    "\n",
    "    if selected_file != \"Select the GeoJSON file with grid cells:\":\n",
    "        file_path = os.path.join(input_path, selected_file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                grid = json.load(file)\n",
    "            display(\"File selected with success:\", selected_file)\n",
    "            display(\"File path:\", file_path)\n",
    "        except FileNotFoundError:\n",
    "            display(\"File not found:\", selected_file)\n",
    "\n",
    "# Listing available GeoJSON files\n",
    "file_list = [f for f in os.listdir(input_path) if f.endswith('.geojson')]\n",
    "options = [\"Select the GeoJSON file with grid cells:\"] + file_list\n",
    "\n",
    "# Dropdown to select the GeoJSON file\n",
    "dropdown = widgets.Dropdown(options=options)\n",
    "dropdown.observe(select_file, names='value')\n",
    "\n",
    "# Display the dropdown\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview grid cells\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86353ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total grid cells in GeoJSON: 8652\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of grid cells in GeoJSON\n",
    "total_cells = len(grid['features'])\n",
    "print(f\"Total grid cells in GeoJSON: {total_cells}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ed344",
   "metadata": {},
   "source": [
    "##### Partition the original GeoJSON grid into subsets of up to 4 cells each to optimise the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b6675a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the original GeoJSON grid into subsets of up to 4 cells each to optimise the process\n",
    "\n",
    "# Number of cells per batch\n",
    "subset_size = 20\n",
    "\n",
    "# Split the original grid cells into subsets\n",
    "subsets = [grid['features'][i:i + subset_size] for i in range(0, len(grid['features']), subset_size)]\n",
    "\n",
    "# Create a new FeatureCollection structure for each subset and add a batch ID (\"lote_id\")\n",
    "grid_subsets = []\n",
    "for index, subset in enumerate(subsets):\n",
    "    grid_subset = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': subset,\n",
    "        'lote_id': f\"lote{index + 1}\",\n",
    "        'crs': grid['crs']\n",
    "    }\n",
    "    grid_subsets.append(grid_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fcb43c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de subsets criados: 433\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of subsets created\n",
    "total_subsets = len(grid_subsets)\n",
    "print(f\"Total de subsets criados: {total_subsets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the grids subsets\n",
    "grid_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5eba0",
   "metadata": {},
   "source": [
    "#### Visualize the spatial distribution of the homogeneous grid cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ed40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to calculate the centroid of a polygon (original grid)\n",
    "def calculate_centroid(coordinates):\n",
    "    x = [p[0] for p in coordinates]\n",
    "    y = [p[1] for p in coordinates]\n",
    "    centroid_x = sum(x) / len(coordinates)\n",
    "    centroid_y = sum(y) / len(coordinates)\n",
    "    return [centroid_y, centroid_x]\n",
    "\n",
    "# Calculate the coordinates of the centroid of the original grid\n",
    "first_polygon = grid['features'][0]['geometry']['coordinates'][0][0]\n",
    "centroid_coords = calculate_centroid(first_polygon)\n",
    "\n",
    "# Function to plot a subset\n",
    "def plot_subset(subset_index):\n",
    "    subset_to_plot = grid_subsets[subset_index]\n",
    "\n",
    "    # GeoJson style\n",
    "    style = {'fillColor': '#8C8989', 'color': '#e31a1c', 'weight': 2}\n",
    "\n",
    "    # Initialize the Folium map at the centroid of the original grid\n",
    "    m = folium.Map(location=centroid_coords, tiles='OpenStreetMap', zoom_start=14)\n",
    "\n",
    "    # Add GeoJson to the map\n",
    "    folium.GeoJson(\n",
    "        subset_to_plot,\n",
    "        name=f'Grade Estatística 200m - Lote {subset_index+1}',\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['id', 'POP10']),\n",
    "        style_function=lambda x: style\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    display(m)\n",
    "\n",
    "# Create the drop-down list with the subset indexes\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[(f'Lote {i+1}', i) for i in range(len(grid_subsets))],\n",
    "    description='Select a Batch:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Update the map based on the selection\n",
    "widgets.interactive(plot_subset, subset_index=dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab4c21",
   "metadata": {},
   "source": [
    "### **OHSOME API**\n",
    "\n",
    " - Access to features, attributes and OSM history edits using the OHSOME API (*OpenStreetMap History Data Analytics Platform*)\n",
    "\n",
    "> - https://docs.ohsome.org/ohsome-api/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15e6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attribution': {'url': 'https://ohsome.org/copyrights',\n",
       "  'text': '© OpenStreetMap contributors'},\n",
       " 'apiVersion': '1.10.4',\n",
       " 'timeout': 600.0,\n",
       " 'extractRegion': {'spatialExtent': {'type': 'Polygon',\n",
       "   'coordinates': [[[-180.0, -90.0],\n",
       "     [180.0, -90.0],\n",
       "     [180.0, 90.0],\n",
       "     [-180.0, 90.0],\n",
       "     [-180.0, -90.0]]]},\n",
       "  'temporalExtent': {'fromTimestamp': '2007-10-08T00:00:00Z',\n",
       "   'toTimestamp': '2025-04-06T13:00Z'},\n",
       "  'replicationSequenceNumber': 110142}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch metadata from the ohsome API\n",
    "# This code fetches metadata from the ohsome API and handles potential JSON decoding errors.\n",
    "import requests\n",
    "\n",
    "URL = 'https://api.ohsome.org/v1/metadata'\n",
    "response = requests.get(URL)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()\n",
    "        print(\"Dados recebidos:\")\n",
    "        display(data)\n",
    "    except ValueError:\n",
    "        print(\"Erro ao decodificar JSON. Conteúdo bruto:\")\n",
    "        display(response.text)\n",
    "else:\n",
    "    display(f\"Erro HTTP {response.status_code}\")\n",
    "    print(\"Resposta:\")\n",
    "    display(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671b8df",
   "metadata": {},
   "source": [
    "### Retrieving data from OpenStreetMap using OHSOME API and homogeneous grid cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e6467",
   "metadata": {},
   "source": [
    "#### Define ET-EDGV class dictionary with respective OSM tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0bb9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novo dicionário de classes ET-EDGV com respectivas tags OSM\n",
    "classe_et_edgv_to_tags = {\n",
    "    'edif_ensino': [\n",
    "        ('amenity', 'school'), ('amenity', 'university'),\n",
    "        ('building', 'school'), ('amenity', 'kindergarten')\n",
    "    ],\n",
    "    'edif_saude': [\n",
    "        ('amenity', 'hospital'), ('amenity', 'clinic'),\n",
    "        ('building', 'hospital'), ('amenity', 'doctors'),\n",
    "        ('amenity', 'dentist'), ('healthcare', '*')\n",
    "    ],\n",
    "    'edif_desenv_social': [\n",
    "        ('amenity', 'social_facility'), ('building', 'public'),\n",
    "        ('social_facility', '*')\n",
    "    ],\n",
    "    'edif_constr_lazer': [\n",
    "        ('leisure', 'park'), ('leisure', 'sports_centre'),\n",
    "        ('leisure', 'stadium'), ('amenity', 'theatre'),\n",
    "        ('amenity', 'library'), ('amenity', 'community_centre'),\n",
    "        ('amenity', 'arts_centre'), ('amenity', 'planetarium'),\n",
    "        ('building', 'grandstand'), ('building', 'stadium'),\n",
    "        ('tourism', 'museum')\n",
    "    ],\n",
    "    'edif_pub_civil': [\n",
    "        ('building', 'public'), ('amenity', 'townhall'),\n",
    "        ('office', 'government')\n",
    "    ],\n",
    "    'edif_turistica': [\n",
    "        ('tourism', 'attraction'), ('tourism', 'artwork'),\n",
    "        ('tourism', 'viewpoint'), ('amenity', 'fountain'),\n",
    "        ('building', 'hotel')\n",
    "    ],\n",
    "    'edif_metro_ferroviaria': [\n",
    "        ('railway', 'station'), ('railway', 'halt'),\n",
    "        ('building', 'train_station'), ('public_transport', 'station')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64442c",
   "metadata": {},
   "source": [
    "#### Step 4 (*API Endpoint: Contributions Aggregation*): Count the total number of contributions to features with a filled-in name where a tagChange occurred:\n",
    "\n",
    "- Count the total number of contributions to the tags of interest, aggregated by grid cell, with the attribute name filled in, considering the type of contribution (contributionType) tag change ('tagChange').\n",
    "\n",
    "  - *contributionType available: ‘creation’, ‘deletion’, ‘tagChange’, ‘geometryChange’ ou uma combinação destes*\n",
    "\n",
    "- Period of data retrieved: 2007-10-08 to 2025-04-06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998190bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: tagChange com name=* — Versão final com paralelização, log CSV, CRS\n",
    "\n",
    "# === CONFIGURAÇÕES GERAIS ===\n",
    "url_tagchange = \"https://api.ohsome.org/v1/contributions/count/groupBy/boundary\"\n",
    "params_tagchange_base = {\n",
    "    'time': '2007-10-08/2025-04-06',\n",
    "    'contributionType': 'tagChange'\n",
    "}\n",
    "\n",
    "output_dir = Path(\"data\\output_code1\\step4_tagchange_name\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = output_dir / \"log_step4.csv\"\n",
    "ultimo_lote_path = output_dir / \"ultimo_lote_step4.txt\"\n",
    "\n",
    "# === INICIALIZAÇÃO DO LOG ===\n",
    "if not log_path.exists():\n",
    "    with open(log_path, 'w', newline='') as f:\n",
    "        csv.writer(f).writerow([\"lote\", \"mensagem\", \"timestamp\"])\n",
    "\n",
    "def log_mensagem(lote, mensagem):\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(log_path, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([lote, mensagem, timestamp])\n",
    "\n",
    "# === KEEP ALIVE ===\n",
    "def keep_alive():\n",
    "    while True:\n",
    "        time.sleep(300)\n",
    "        print(\"Ainda trabalhando...\")\n",
    "        log_mensagem(\"keep_alive\", \"Ainda trabalhando...\")\n",
    "\n",
    "threading.Thread(target=keep_alive, daemon=True).start()\n",
    "\n",
    "# === FUNÇÃO DE CONTAGEM ===\n",
    "def contar_tagchange_name(cell_geojson, tag, value):\n",
    "    try:\n",
    "        params = params_tagchange_base.copy()\n",
    "        params.update({'bpolys': cell_geojson, 'filter': f'{tag}={value} and name=*'})\n",
    "        resp = requests.post(url_tagchange, data=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return sum(r.get('value', 0) for r in data.get('groupByResult', [])[0].get('result', []))\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "# === PROCESSAMENTO DE UMA CÉLULA ===\n",
    "def process_tagchange_cell(feature):\n",
    "    new_feature = json.loads(json.dumps(feature))  # Deep copy\n",
    "    cell_geojson = json.dumps({\"type\": \"FeatureCollection\", \"features\": [new_feature]})\n",
    "    cell_id = new_feature['properties']['id']\n",
    "\n",
    "    for classe, tag_list in classe_et_edgv_to_tags.items():\n",
    "        total_tagchange = 0\n",
    "\n",
    "        for tag, value in tag_list:\n",
    "            for attempt in range(3):\n",
    "                resultado = contar_tagchange_name(cell_geojson, tag, value)\n",
    "                if resultado is not None:\n",
    "                    break\n",
    "                time.sleep(2 ** attempt)\n",
    "\n",
    "            if resultado is None:\n",
    "                print(f\"[ERRO TAGCHANGE] Célula {cell_id} | Tag: {tag}={value}\")\n",
    "                log_mensagem(cell_id, f\"ERRO {tag}={value}\")\n",
    "                continue\n",
    "\n",
    "            total_tagchange += resultado\n",
    "\n",
    "        new_feature['properties'][f'{classe}_name_tagchange'] = total_tagchange\n",
    "\n",
    "    return cell_id, new_feature\n",
    "\n",
    "# === EXECUÇÃO DOS LOTES ===\n",
    "ultimo_lote = 0\n",
    "if ultimo_lote_path.exists():\n",
    "    with open(ultimo_lote_path, 'r') as f:\n",
    "        ultimo_lote = int(f.read().strip())\n",
    "\n",
    "for lote_index in range(ultimo_lote, len(grid_subsets)):\n",
    "    start_time = time.time()\n",
    "    subset = grid_subsets[lote_index]\n",
    "    feature_list = subset['features']\n",
    "\n",
    "    updated_features = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(process_tagchange_cell, f) for f in feature_list]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Lote {lote_index + 1} (Step 4)\"):\n",
    "            try:\n",
    "                _, processed_feature = future.result()\n",
    "                updated_features.append(processed_feature)\n",
    "            except Exception as e:\n",
    "                log_mensagem(lote_index + 1, f\"FALHA: {e}\")\n",
    "\n",
    "    fc = {\"type\": \"FeatureCollection\", \"features\": updated_features}\n",
    "    if 'crs' in grid_subsets[0]:\n",
    "        fc['crs'] = grid_subsets[0]['crs']\n",
    "\n",
    "    out_path = output_dir / f\"step4_lote{lote_index + 1}.geojson\"\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(fc, f)\n",
    "    log_mensagem(lote_index + 1, f\"SALVO {out_path.name}\")\n",
    "\n",
    "    # === CONSOLIDAÇÃO PARCIAL ===\n",
    "    arquivos = sorted(glob.glob(str(output_dir / \"step4_lote*.geojson\")))\n",
    "    todas_features = []\n",
    "    for arquivo in arquivos:\n",
    "        with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "            fc_parcial = json.load(f)\n",
    "            todas_features.extend(fc_parcial['features'])\n",
    "\n",
    "    final_fc = {\"type\": \"FeatureCollection\", \"features\": todas_features}\n",
    "    if 'crs' in grid_subsets[0]:\n",
    "        final_fc['crs'] = grid_subsets[0]['crs']\n",
    "\n",
    "    with open(output_dir / \"step4_consolidado.geojson\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_fc, f)\n",
    "    print(\"[CONSOLIDADO] step4_consolidado.geojson atualizado\")\n",
    "    log_mensagem(lote_index + 1, \"CONSOLIDADO atualizado\")\n",
    "\n",
    "    with open(ultimo_lote_path, 'w') as f:\n",
    "        f.write(str(lote_index + 1))\n",
    "\n",
    "    tempo_msg = f\"Tempo lote {lote_index + 1}: {str(timedelta(seconds=int(time.time() - start_time)))}\"\n",
    "    print(tempo_msg)\n",
    "    log_mensagem(lote_index + 1, tempo_msg)\n",
    "\n",
    "print(\"Step 4 (tagChange com name) finalizado com sucesso.\")\n",
    "log_mensagem(\"step4\", \"Processamento finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a87db",
   "metadata": {},
   "source": [
    "#### Step 5 (API Endpoint: Users Aggregation): Count the number of users (contributors) who edited features with attribute name filled in:\n",
    "\n",
    "- Count the number of users who edited features of the OSM tags of Interest with attribute \"name\" attribute filled in, aggregated by grid cells.\n",
    "\n",
    "- Period of data retrieved: 2007-10-08 to 2025-04-06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Users with name=* — Versão final com paralelização, logs, retry, crs\n",
    "\n",
    "# === CONFIGURAÇÕES GERAIS ===\n",
    "url_users = \"https://api.ohsome.org/v1/users/count/groupBy/boundary\"\n",
    "params_users_base = {'time': '2007-10-08/2025-04-06'}\n",
    "\n",
    "output_dir = Path(\"data\\output_code1\\step5_users_name\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = output_dir / \"log_step5.csv\"\n",
    "ultimo_lote_path = output_dir / \"ultimo_lote_step5.txt\"\n",
    "\n",
    "# === INICIALIZAÇÃO DO LOG ===\n",
    "if not log_path.exists():\n",
    "    with open(log_path, 'w', newline='') as f:\n",
    "        csv.writer(f).writerow([\"lote\", \"mensagem\", \"timestamp\"])\n",
    "\n",
    "def log_mensagem(lote, mensagem):\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(log_path, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([lote, mensagem, timestamp])\n",
    "\n",
    "# === KEEP ALIVE ===\n",
    "def keep_alive():\n",
    "    while True:\n",
    "        time.sleep(300)\n",
    "        print(\"Ainda trabalhando...\")\n",
    "        log_mensagem(\"keep_alive\", \"Ainda trabalhando...\")\n",
    "\n",
    "threading.Thread(target=keep_alive, daemon=True).start()\n",
    "\n",
    "# === FUNÇÃO DE CONTAGEM DE USUÁRIOS COM NAME ===\n",
    "def contar_usuarios_name(cell_geojson, tag, value):\n",
    "    try:\n",
    "        params = params_users_base.copy()\n",
    "        params.update({'bpolys': cell_geojson, 'filter': f'{tag}={value} and name=*'})\n",
    "        resp = requests.post(url_users, data=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return sum(r.get('value', 0) for r in data.get('groupByResult', [])[0].get('result', []))\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "# === PROCESSAMENTO DE UMA CÉLULA ===\n",
    "def process_users_cell(feature):\n",
    "    new_feature = json.loads(json.dumps(feature))  # deep copy\n",
    "    cell_geojson = json.dumps({\"type\": \"FeatureCollection\", \"features\": [new_feature]})\n",
    "    cell_id = new_feature['properties']['id']\n",
    "\n",
    "    for classe, tag_list in classe_et_edgv_to_tags.items():\n",
    "        total_users_name = 0\n",
    "\n",
    "        for tag, value in tag_list:\n",
    "            for attempt in range(3):\n",
    "                resultado = contar_usuarios_name(cell_geojson, tag, value)\n",
    "                if resultado is not None:\n",
    "                    break\n",
    "                time.sleep(2 ** attempt)\n",
    "\n",
    "            if resultado is None:\n",
    "                print(f\"[ERRO USERS] Célula {cell_id} | Tag: {tag}={value}\")\n",
    "                log_mensagem(cell_id, f\"ERRO {tag}={value}\")\n",
    "                continue\n",
    "\n",
    "            total_users_name += resultado\n",
    "\n",
    "        new_feature['properties'][f'{classe}_users_name'] = total_users_name\n",
    "\n",
    "    return cell_id, new_feature\n",
    "\n",
    "# === EXECUÇÃO DOS LOTES ===\n",
    "ultimo_lote = 0\n",
    "if ultimo_lote_path.exists():\n",
    "    with open(ultimo_lote_path, 'r') as f:\n",
    "        ultimo_lote = int(f.read().strip())\n",
    "\n",
    "for lote_index in range(ultimo_lote, len(grid_subsets)):\n",
    "    start_time = time.time()\n",
    "    subset = grid_subsets[lote_index]\n",
    "    feature_list = subset['features']\n",
    "\n",
    "    updated_features = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(process_users_cell, f) for f in feature_list]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Lote {lote_index + 1} (Step 5)\"):\n",
    "            try:\n",
    "                _, processed_feature = future.result()\n",
    "                updated_features.append(processed_feature)\n",
    "            except Exception as e:\n",
    "                log_mensagem(lote_index + 1, f\"FALHA: {e}\")\n",
    "\n",
    "    fc = {\"type\": \"FeatureCollection\", \"features\": updated_features}\n",
    "    if 'crs' in grid_subsets[0]:\n",
    "        fc['crs'] = grid_subsets[0]['crs']\n",
    "\n",
    "    out_path = output_dir / f\"step5_lote{lote_index + 1}.geojson\"\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(fc, f)\n",
    "    log_mensagem(lote_index + 1, f\"SALVO {out_path.name}\")\n",
    "\n",
    "    # === CONSOLIDAÇÃO PARCIAL ===\n",
    "    arquivos = sorted(glob.glob(str(output_dir / \"step5_lote*.geojson\")))\n",
    "    todas_features = []\n",
    "    for arquivo in arquivos:\n",
    "        with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "            fc_parcial = json.load(f)\n",
    "            todas_features.extend(fc_parcial['features'])\n",
    "\n",
    "    final_fc = {\"type\": \"FeatureCollection\", \"features\": todas_features}\n",
    "    if 'crs' in grid_subsets[0]:\n",
    "        final_fc['crs'] = grid_subsets[0]['crs']\n",
    "\n",
    "    with open(output_dir / \"step5_consolidado.geojson\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_fc, f)\n",
    "    print(\"[CONSOLIDADO] step5_consolidado.geojson atualizado\")\n",
    "    log_mensagem(lote_index + 1, \"CONSOLIDADO atualizado\")\n",
    "\n",
    "    with open(ultimo_lote_path, 'w') as f:\n",
    "        f.write(str(lote_index + 1))\n",
    "\n",
    "    tempo_msg = f\"Tempo lote {lote_index + 1}: {str(timedelta(seconds=int(time.time() - start_time)))}\"\n",
    "    print(tempo_msg)\n",
    "    log_mensagem(lote_index + 1, tempo_msg)\n",
    "\n",
    "print(\"Step 5 (Users with name) finalizado com sucesso.\")\n",
    "log_mensagem(\"step5\", \"Processamento finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84456555",
   "metadata": {},
   "source": [
    "#### Local tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85224de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHSOME API endpoint url\n",
    "url_tag = \"https://api.ohsome.org/v1/elements/count/groupBy/boundary/groupBy/tag\"\n",
    "\n",
    "\n",
    "# Start the time counter\n",
    "start_time = time.time()\n",
    "\n",
    "# Load a copy of previously created grid_subsets\n",
    "grid_subset2 = grid_subsets.copy()\n",
    "\n",
    "# Configuring basic parameters\n",
    "params_base = {\n",
    "    'time': '2007-10-08/2025-04-06'\n",
    "}\n",
    "\n",
    "# List to store the final results\n",
    "final_results = {}\n",
    "\n",
    "# Processar lotes\n",
    "for lote_id, subset in enumerate(grid_subset2, start=1):\n",
    "    for feature in subset['features']:\n",
    "        cell_geojson = json.dumps({\"type\": \"FeatureCollection\", \"features\": [feature]})\n",
    "        cell_id = feature['properties']['id']\n",
    "\n",
    "        for classe, tag_list in classe_et_edgv_to_tags.items():\n",
    "            total_count_classe = 0\n",
    "            name_count_classe = 0\n",
    "            erro_detectado = False\n",
    "\n",
    "            for tag, value in tag_list:\n",
    "                try:\n",
    "                    params_total = params_base.copy()\n",
    "                    params_total.update({\n",
    "                        'bpolys': cell_geojson,\n",
    "                        'filter': f'{tag}={value}',\n",
    "                        'groupByKey': tag,\n",
    "                        'groupByValues': value\n",
    "                    })\n",
    "\n",
    "                    response = requests.post(url_tag, data=params_total)\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "                    count = sum(res.get('value', 0) for res in data.get('groupByResult', [])[0].get('result', []))\n",
    "                    total_count_classe += count\n",
    "\n",
    "                    # Contar apenas com nome preenchido\n",
    "                    params_name = params_total.copy()\n",
    "                    params_name['filter'] = f'{tag}={value} and name=*'\n",
    "                    response = requests.post(url_tag, data=params_name)\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "                    name_count = sum(res.get('value', 0) for res in data.get('groupByResult', [])[0].get('result', []))\n",
    "                    name_count_classe += name_count\n",
    "\n",
    "                except Exception as e:\n",
    "                    erro_detectado = True\n",
    "                    print(f\"[ERRO] Célula {cell_id} | Tag: {tag}={value} | Erro: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Armazenar resultados por classe ET-EDGV\n",
    "            feature['properties'][f'{classe}_total_count'] = total_count_classe if not erro_detectado else 0\n",
    "            feature['properties'][f'{classe}_name_count'] = name_count_classe if not erro_detectado else 0\n",
    "            feature['properties'][f'{classe}_name_ratio'] = (\n",
    "                (name_count_classe / total_count_classe) * 100 if total_count_classe > 0 else 0\n",
    "            )\n",
    "\n",
    "        final_results[cell_id] = feature['properties']\n",
    "\n",
    "    print(f\"Lote {lote_id} processado com sucesso!\")\n",
    "\n",
    "# Tempo total de execução\n",
    "end_time = time.time()\n",
    "total_time_seconds = end_time - start_time\n",
    "print(f\"Tempo total: {total_time_seconds // 60} minutos e {total_time_seconds % 60:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92627622",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"../outputs/OHSOME_api/02_testes_feicoes_agregadasGrade/bh/grade_id36_bh_12cells_results1_local.geojson\"\n",
    "# Create a new FeatureCollection to combine all the subsets\n",
    "grid_subset2_results = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'crs': grid_subset2[0]['crs'],\n",
    "    'features': []\n",
    "}\n",
    "\n",
    "# Iterate over each subset and add its features to the combined FeatureCollection\n",
    "for subset in grid_subset2:\n",
    "    grid_subset2_results['features'].extend(subset['features'])\n",
    "\n",
    "# Save the combined grid cells in a GeoJSON file\n",
    "with open(output_filename, 'w') as file:\n",
    "    json.dump(grid_subset2_results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-DScPythonGeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
