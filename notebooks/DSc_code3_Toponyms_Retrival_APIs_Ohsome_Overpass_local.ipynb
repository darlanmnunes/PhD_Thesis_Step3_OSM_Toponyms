{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860fe2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library and some pre-installed modules\n",
    "import os\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd310426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the root directory of the project as the working directory\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36379e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/darlanmnunes/Dev/DSc_git/PhD_Thesis_Step3_OSM_Toponyms'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current working directory\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e705ee5",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5adb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.processar_com_ohsome' from '/Users/darlanmnunes/Dev/DSc_git/PhD_Thesis_Step3_OSM_Toponyms/src/processar_com_ohsome.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the utils and processar_com_overpass modules to ensure any changes are reflected\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "import src.utils as utils\n",
    "import src.processar_com_overpass as processar_com_overpass\n",
    "import src.processar_com_ohsome as processar_com_ohsome\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(processar_com_overpass)\n",
    "importlib.reload(processar_com_ohsome)\n",
    "\n",
    "#Alternativa\n",
    "#importlib.reload(sys.modules[\"src.utils\"])\n",
    "#importlib.reload(sys.modules[\"src.processar_com_overpass\"])\n",
    "#importlib.reload(sys.modules[\"src.processar_com_ohsome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671b8df",
   "metadata": {},
   "source": [
    "## Retrieving data from OpenStreetMap using APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e6467",
   "metadata": {},
   "source": [
    "### Define ET-EDGV class dictionary with respective OSM tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0bb9e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edif_ensino': [('amenity', 'school'),\n",
       "  ('amenity', 'university'),\n",
       "  ('building', 'school'),\n",
       "  ('amenity', 'kindergarten')],\n",
       " 'edif_saude': [('amenity', 'hospital'),\n",
       "  ('amenity', 'clinic'),\n",
       "  ('building', 'hospital'),\n",
       "  ('amenity', 'doctors'),\n",
       "  ('amenity', 'dentist'),\n",
       "  ('healthcare', '*')],\n",
       " 'edif_desenv_social': [('amenity', 'social_facility'),\n",
       "  ('building', 'public'),\n",
       "  ('social_facility', '*')],\n",
       " 'edif_constr_lazer': [('leisure', 'park'),\n",
       "  ('leisure', 'sports_centre'),\n",
       "  ('leisure', 'stadium'),\n",
       "  ('amenity', 'theatre'),\n",
       "  ('amenity', 'library'),\n",
       "  ('amenity', 'community_centre'),\n",
       "  ('amenity', 'arts_centre'),\n",
       "  ('amenity', 'planetarium'),\n",
       "  ('building', 'grandstand'),\n",
       "  ('building', 'stadium'),\n",
       "  ('tourism', 'museum')],\n",
       " 'edif_pub_civil': [('building', 'public'),\n",
       "  ('amenity', 'townhall'),\n",
       "  ('office', 'government')],\n",
       " 'edif_turistica': [('tourism', 'attraction'),\n",
       "  ('tourism', 'artwork'),\n",
       "  ('tourism', 'viewpoint'),\n",
       "  ('amenity', 'fountain'),\n",
       "  ('building', 'hotel')],\n",
       " 'edif_metro_ferroviaria': [('railway', 'station'),\n",
       "  ('railway', 'halt'),\n",
       "  ('building', 'train_station'),\n",
       "  ('public_transport', 'station')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Novo dicionário de classes ET-EDGV com respectivas tags OSM\n",
    "classe_et_edgv_to_tags = {\n",
    "    'edif_ensino': [\n",
    "        ('amenity', 'school'), ('amenity', 'university'),\n",
    "        ('building', 'school'), ('amenity', 'kindergarten')\n",
    "    ],\n",
    "    'edif_saude': [\n",
    "        ('amenity', 'hospital'), ('amenity', 'clinic'),\n",
    "        ('building', 'hospital'), ('amenity', 'doctors'),\n",
    "        ('amenity', 'dentist'), ('healthcare', '*')\n",
    "    ],\n",
    "    'edif_desenv_social': [\n",
    "        ('amenity', 'social_facility'), ('building', 'public'),\n",
    "        ('social_facility', '*')\n",
    "    ],\n",
    "    'edif_constr_lazer': [\n",
    "        ('leisure', 'park'), ('leisure', 'sports_centre'),\n",
    "        ('leisure', 'stadium'), ('amenity', 'theatre'),\n",
    "        ('amenity', 'library'), ('amenity', 'community_centre'),\n",
    "        ('amenity', 'arts_centre'), ('amenity', 'planetarium'),\n",
    "        ('building', 'grandstand'), ('building', 'stadium'),\n",
    "        ('tourism', 'museum')\n",
    "    ],\n",
    "    'edif_pub_civil': [\n",
    "        ('building', 'public'), ('amenity', 'townhall'),\n",
    "        ('office', 'government')\n",
    "    ],\n",
    "    'edif_turistica': [\n",
    "        ('tourism', 'attraction'), ('tourism', 'artwork'),\n",
    "        ('tourism', 'viewpoint'), ('amenity', 'fountain'),\n",
    "        ('building', 'hotel')\n",
    "    ],\n",
    "    'edif_metro_ferroviaria': [\n",
    "        ('railway', 'station'), ('railway', 'halt'),\n",
    "        ('building', 'train_station'), ('public_transport', 'station')\n",
    "    ]\n",
    "}\n",
    "classe_et_edgv_to_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64442c",
   "metadata": {},
   "source": [
    "### Step 7 - Retrieval of the last toponyms by the most recent feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdb58d",
   "metadata": {},
   "source": [
    "#### PostGIS - Open the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10b2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão ao Banco PostGIS\n",
    "import psycopg2\n",
    "\n",
    "# Function to load database credentials from a text file\n",
    "def load_credentials_from_txt(file_path):\n",
    "    credentials = {}\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if '=' in line:\n",
    "                    key, value = line.strip().split('=', 1)\n",
    "                    credentials[key.strip()] = value.strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de credenciais não encontrado: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler credenciais: {e}\")\n",
    "    return credentials\n",
    "\n",
    "def connect_to_postgis(txt_path='configs/db_credentials.txt'):\n",
    "    creds = load_credentials_from_txt(txt_path)\n",
    "\n",
    "    required_keys = ['DB_NAME', 'DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT']\n",
    "    if not all(k in creds for k in required_keys):\n",
    "        print(\"Credenciais incompletas no arquivo de configuração.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=creds['DB_NAME'],\n",
    "            user=creds['DB_USER'],\n",
    "            password=creds['DB_PASSWORD'],\n",
    "            host=creds['DB_HOST'],\n",
    "            port=creds['DB_PORT']\n",
    "        )\n",
    "        print(\"Conexão ao PostGIS estabelecida com sucesso!\")\n",
    "        return conn\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao conectar ao PostGIS: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ec3843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão ao PostGIS estabelecida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Open the database connection\n",
    "conn = connect_to_postgis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004a37a",
   "metadata": {},
   "source": [
    "#### Filter grid cells from the database with name_ratio > 0\n",
    "\n",
    " * Pré-filtragem SQL\n",
    " * Garantir que pelo menos uma classe tem name_ratio > 0 para cada célula da grade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685267f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch valid grid cells from the database\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "def fetch_geometries_psycopg2(conn, classe_et_edgv_to_tags, table_name):\n",
    "    \"\"\"\n",
    "    Recupera geometrias do PostGIS com base nos filtros name_ratio > 0 usando psycopg2.\n",
    "    Retorna um GeoDataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Garante que qualquer erro anterior seja limpo\n",
    "        conn.rollback()\n",
    "\n",
    "        # Monta cláusula WHERE\n",
    "        where_clause = \" OR \".join([\n",
    "            f\"step1_consolidado_{classe}_name_ratio > 0\" for classe in classe_et_edgv_to_tags\n",
    "        ])\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *, ST_AsText(geom) AS wkt_geom\n",
    "            FROM public.{table_name}\n",
    "            WHERE {where_clause}\n",
    "        \"\"\"\n",
    "\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            colnames = [desc[0] for desc in cur.description]\n",
    "            rows = cur.fetchall()\n",
    "\n",
    "        # Monta DataFrame\n",
    "        df = pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "        # Converte geometria WKT em shapely\n",
    "        df[\"geometry\"] = df[\"wkt_geom\"].apply(wkt.loads)\n",
    "        gdf = gpd.GeoDataFrame(df.drop(columns=[\"wkt_geom\"]), geometry=\"geometry\")\n",
    "\n",
    "        # Define CRS padrão\n",
    "        gdf.set_crs(epsg=4674, inplace=True)\n",
    "\n",
    "        print(f\"Consulta retornou {len(gdf)} registros.\")\n",
    "        return gdf\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        conn.rollback()\n",
    "        print(\"Erro ao executar a consulta SQL:\")\n",
    "        print(e.pgerror)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the valid grid cell (name_ratio>0)from the database using psycopg2\n",
    "gdf_cells_valid = fetch_geometries_psycopg2(conn, classe_et_edgv_to_tags, table_name=\"steps_consolidado_20cells_tests\")\n",
    "display(gdf_cells_valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se as colunas estão corretas\n",
    "gdf_cells_valid.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4823a98",
   "metadata": {},
   "source": [
    "#### **Request last toponyms and metadata – OHSOME API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386beb1f",
   "metadata": {},
   "source": [
    "\n",
    "1. Conectar ao PostGIS para filtrar apenas células onde name_ratio > 0 para pelo menos uma classe.\n",
    "\n",
    "2. Pré-filtragem das por células das classe com as tags do dicionário classe_et_edgv_to_tags - aumentar performance.\n",
    "\n",
    "3. Para cada célula e classe com name_ratio > 0:\n",
    "\n",
    "  * Extrair bbox da célula válida;\n",
    "\n",
    "  * Uso do inflexao_data (step6) como início da janela temporal\n",
    "\n",
    "  * Fazer chamada A API OSHOME para para recuperar a contribuição mais recente com name=*\n",
    "    - Endpoint: POST /contributions/latest/geometry;\n",
    "\n",
    "  * Respeito à data de inflexão (step6_consolidado_{classe}_inflexao_data)\n",
    "\n",
    "  * Resgatar geometria (pontos) e metadados:\n",
    "    - Metadados: @timestamp, @osmId, tags, name.\n",
    "\n",
    "4. Paralelização por célula\n",
    "\n",
    "5. Log detalhado e consolidação final\n",
    "\n",
    "6. Salvar os resultados em GeoJSON incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdb47c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados recebidos:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attribution': {'url': 'https://ohsome.org/copyrights',\n",
       "  'text': '© OpenStreetMap contributors'},\n",
       " 'apiVersion': '1.10.4',\n",
       " 'timeout': 600.0,\n",
       " 'extractRegion': {'spatialExtent': {'type': 'Polygon',\n",
       "   'coordinates': [[[-180.0, -90.0],\n",
       "     [180.0, -90.0],\n",
       "     [180.0, 90.0],\n",
       "     [-180.0, 90.0],\n",
       "     [-180.0, -90.0]]]},\n",
       "  'temporalExtent': {'fromTimestamp': '2007-10-08T00:00:00Z',\n",
       "   'toTimestamp': '2025-04-06T13:00Z'},\n",
       "  'replicationSequenceNumber': 110142}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch metadata from the ohsome API\n",
    "# This code fetches metadata from the ohsome API and handles potential JSON decoding errors.\n",
    "import requests\n",
    "\n",
    "URL = 'https://api.ohsome.org/v1/metadata'\n",
    "response = requests.get(URL)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()\n",
    "        print(\"Dados recebidos:\")\n",
    "        display(data)\n",
    "    except ValueError:\n",
    "        print(\"Erro ao decodificar JSON. Conteúdo bruto:\")\n",
    "        display(response.text)\n",
    "else:\n",
    "    display(f\"Erro HTTP {response.status_code}\")\n",
    "    print(\"Resposta:\")\n",
    "    display(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4b05b",
   "metadata": {},
   "source": [
    "##### Implementação sem módulos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7: Último topônimo OSM por classe usando a API OHSOME ===\n",
    "# Versão final com paralelização, logs, output GeoJSON e metadados completos\n",
    "\n",
    "# Sem módulos externos (Utils), apenas bibliotecas padrão e geopandas\n",
    "\n",
    "# === Import necessary libraries ===\n",
    "import requests\n",
    "import json\n",
    "from shapely.geometry import shape, mapping\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import timedelta\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# === CONFIGURAÇÕES GERAIS ===\n",
    "output_dir = Path(\"data/output_code1/20cells_tests/step7_latest_name\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = output_dir / \"log_step7.csv\"\n",
    "ultimo_lote_path = output_dir / \"ultimo_lote_step7.txt\"\n",
    "url_ohsome_latest = \"https://api.ohsome.org/v1/contributions/latest/geometry\"\n",
    "\n",
    "# === INICIALIZAÇÃO DO LOG ===\n",
    "if not log_path.exists():\n",
    "    with open(log_path, 'w', newline='') as f:\n",
    "        csv.writer(f).writerow([\"lote\", \"mensagem\", \"timestamp\"])\n",
    "\n",
    "def log_mensagem(lote, mensagem):\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(log_path, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([lote, mensagem, timestamp])\n",
    "\n",
    "# === KEEP ALIVE (com controle de término) ===\n",
    "# Flag global para controle\n",
    "keep_alive_running = True\n",
    "\n",
    "def keep_alive():\n",
    "    while keep_alive_running:\n",
    "        time.sleep(300)\n",
    "        print(\"Ainda trabalhando...\")\n",
    "        log_mensagem(\"keep_alive\", \"Ainda trabalhando...\")\n",
    "\n",
    "keep_alive_thread = threading.Thread(target=keep_alive, daemon=True)\n",
    "keep_alive_thread.start()\n",
    "\n",
    "# === FUNÇÃO PARA PROCESSAR UMA CÉLULA ===\n",
    "def processar_ultima_contribuicao(cell_row):\n",
    "    id_celula = cell_row[\"id\"]\n",
    "    bbox = cell_row.geometry.bounds  # (minx, miny, maxx, maxy)\n",
    "    features_resultantes = []\n",
    "\n",
    "    for classe, tags in classe_et_edgv_to_tags.items():\n",
    "        ratio_col = f\"step1_consolidado_{classe}_name_ratio\"\n",
    "        inflexao_col = f\"step6_consolidado_{classe}_inflexao_data\"\n",
    "\n",
    "        if pd.isna(cell_row.get(ratio_col)) or cell_row[ratio_col] <= 0:\n",
    "            continue\n",
    "\n",
    "        data_inicio_str = cell_row.get(inflexao_col) # Uso do inflexao_data como início da janela temporal\n",
    "        if not isinstance(data_inicio_str, str) or data_inicio_str.strip() == \"\" or data_inicio_str.lower() == \"none\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data_inicio = pd.to_datetime(data_inicio_str, errors=\"coerce\")\n",
    "            if pd.isna(data_inicio):\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            log_mensagem(id_celula, f\"[ERRO] Conversão de data inflexão inválida: {data_inicio_str} - {e}\")\n",
    "            continue\n",
    "\n",
    "        data_fim = pd.Timestamp(\"2025-04-06T13:00Z\").strftime(\"%Y-%m-%d\") # data_fim fixa de acordo com API metadata ('temporalExtent')\n",
    "        data_inicio_str = data_inicio.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        for tag, value in tags:\n",
    "            payload = {\n",
    "                \"bboxes\": f\"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}\",\n",
    "                \"time\": f\"{data_inicio_str},{data_fim}\",\n",
    "                \"filter\": f\"{tag}={value} and name=*\",\n",
    "                \"properties\": \"metadata,tags\",\n",
    "                \"clipGeometry\": \"false\"\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = requests.post(url_ohsome_latest, data=payload)\n",
    "                \n",
    "                print(f\"[DEBUG] Célula: {id_celula}, Classe: {classe}, Tag={tag}, Value={value}\")\n",
    "                print(f\"[DEBUG] Payload: {json.dumps(payload)}\")\n",
    "                print(f\"[DEBUG] Status Code: {response.status_code}\")\n",
    "                print(f\"[DEBUG] Response Text: {response.text[:300]}\")  # apenas os primeiros 300 chars\n",
    "\n",
    "                response.raise_for_status()\n",
    "                dados = response.json()\n",
    "\n",
    "                for feat in dados.get(\"features\", []):\n",
    "                    geom_data = feat.get(\"geometry\")\n",
    "                    props = feat.get(\"properties\", {})\n",
    "\n",
    "                    if geom_data is None:\n",
    "                        continue\n",
    "\n",
    "                    geom = shape(geom_data)\n",
    "                    if geom.geom_type in [\"Polygon\", \"MultiPolygon\"]:\n",
    "                        geom = geom.centroid\n",
    "\n",
    "                    props_clean = {\n",
    "                        \"id_celula\": id_celula,\n",
    "                        \"classe\": classe,\n",
    "                        \"tag\": tag,\n",
    "                        \"value\": value,\n",
    "                        **props\n",
    "                    }\n",
    "\n",
    "                    features_resultantes.append({\n",
    "                        \"type\": \"Feature\",\n",
    "                        \"geometry\": mapping(geom),\n",
    "                        \"properties\": props_clean\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                log_mensagem(id_celula, f\"[ERRO OHSOME {classe}] {tag}={value}: {str(e)}\")\n",
    "\n",
    "    return features_resultantes\n",
    "\n",
    "try:\n",
    "    # === EXECUÇÃO EM LOTE ===\n",
    "    ultimo_lote = 0\n",
    "    if ultimo_lote_path.exists():\n",
    "        with open(ultimo_lote_path, 'r') as f:\n",
    "            ultimo_lote = int(f.read().strip())\n",
    "\n",
    "    lote_size = 20\n",
    "    total_lotes = math.ceil(len(gdf_cells_valid) / lote_size)\n",
    "\n",
    "    for lote_index in range(ultimo_lote, total_lotes):\n",
    "        start_time = time.time()\n",
    "        features_final = []\n",
    "\n",
    "        subset = gdf_cells_valid.iloc[lote_index * lote_size: (lote_index + 1) * lote_size]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(processar_ultima_contribuicao, row) for _, row in subset.iterrows()]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Lote {lote_index + 1} (Step 7)\"):\n",
    "                try:\n",
    "                    features_final.extend(future.result())\n",
    "                except Exception as e:\n",
    "                    log_mensagem(lote_index + 1, f\"[FALHA GERAL]: {e}\")\n",
    "\n",
    "        # Salva lote em GeoJSON\n",
    "        fc = {\"type\": \"FeatureCollection\", \"features\": features_final}\n",
    "        out_path = output_dir / f\"step7_lote{lote_index + 1}.geojson\"\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(fc, f)\n",
    "        log_mensagem(lote_index + 1, f\"SALVO {out_path.name}\")\n",
    "\n",
    "        # Atualiza consolidação incremental\n",
    "        arquivos = sorted(output_dir.glob(\"step7_lote*.geojson\"))\n",
    "        todas_features = []\n",
    "        for arquivo in arquivos:\n",
    "            with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "                fc_parcial = json.load(f)\n",
    "                todas_features.extend(fc_parcial['features'])\n",
    "\n",
    "        final_fc = {\"type\": \"FeatureCollection\", \"features\": todas_features}\n",
    "        with open(output_dir / \"step7_consolidado.geojson\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_fc, f)\n",
    "        log_mensagem(lote_index + 1, \"CONSOLIDADO atualizado\")\n",
    "\n",
    "        with open(ultimo_lote_path, 'w') as f:\n",
    "            f.write(str(lote_index + 1))\n",
    "\n",
    "        tempo_msg = f\"Tempo lote {lote_index + 1}: {str(timedelta(seconds=int(time.time() - start_time)))}\"\n",
    "        print(tempo_msg)\n",
    "        log_mensagem(lote_index + 1, tempo_msg)\n",
    "\n",
    "    print(\"Step 7 (último topônimo por classe) finalizado com sucesso.\")\n",
    "    log_mensagem(\"step7\", \"Processamento finalizado\")\n",
    "\n",
    "finally:\n",
    "    keep_alive_running = False\n",
    "    keep_alive_thread.join(timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a0c0d",
   "metadata": {},
   "source": [
    "##### Implementação com módulos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7 (OHSOME API) ===\n",
    "# Com módulos utils e processar_com_ohsome\n",
    "\n",
    "# === Import necessary libraries and modules ===\n",
    "from src.utils import init_log, start_keep_alive, log_mensagem, consolidar_geojson\n",
    "from src.processar_com_ohsome import processar_com_ohsome\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import timedelta\n",
    "import json, math, time\n",
    "\n",
    "# === CONFIGURAÇÕES GERAIS ===\n",
    "output_dir = Path(\"data/output_code1/step7_latest_name_ohsome\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = output_dir / \"log_step7_ohsome.csv\"\n",
    "ultimo_lote_path = output_dir / \"ultimo_lote_step7_ohsome.txt\"\n",
    "\n",
    "# === LOG E KEEP ALIVE ===\n",
    "# Flag global para controle\n",
    "\n",
    "init_log(log_path)\n",
    "keep_alive_flag = {\"running\": True}\n",
    "keep_alive_thread = start_keep_alive(log_path, keep_alive_flag)\n",
    "\n",
    "# === EXECUÇÃO EM LOTE ===\n",
    "try:\n",
    "    ultimo_lote = int(ultimo_lote_path.read_text().strip()) if ultimo_lote_path.exists() else 0\n",
    "    lote_size = 20\n",
    "    total_lotes = math.ceil(len(gdf_cells_valid) / lote_size)\n",
    "\n",
    "    for lote_index in range(ultimo_lote, total_lotes):\n",
    "        start_time = time.time()\n",
    "        features_final = []\n",
    "        subset = gdf_cells_valid.iloc[lote_index * lote_size: (lote_index + 1) * lote_size]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [\n",
    "                executor.submit(processar_com_ohsome, row, classe_et_edgv_to_tags, log_mensagem, log_path)\n",
    "                for _, row in subset.iterrows()\n",
    "            ]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Lote {lote_index + 1} (Step 7 OHSOME)\"):\n",
    "                try:\n",
    "                    features_final.extend(future.result())\n",
    "                except Exception as e:\n",
    "                    log_mensagem(log_path, lote_index + 1, f\"[FALHA GERAL]: {e}\")\n",
    "\n",
    "        out_path = output_dir / f\"step7_lote{lote_index + 1}.geojson\"\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"type\": \"FeatureCollection\", \"features\": features_final}, f)\n",
    "        log_mensagem(log_path, lote_index + 1, f\"SALVO {out_path.name}\")\n",
    "\n",
    "        total_feats = consolidar_geojson(output_dir, \"step7_lote*.geojson\", \"step7_consolidado.geojson\")\n",
    "        log_mensagem(log_path, lote_index + 1, f\"CONSOLIDADO atualizado: {total_feats} features\")\n",
    "\n",
    "        ultimo_lote_path.write_text(str(lote_index + 1))\n",
    "        print(f\"Lote {lote_index + 1} concluído em {str(timedelta(seconds=int(time.time() - start_time)))}\")\n",
    "\n",
    "    print(\"Step 7 (OHSOME) finalizado com sucesso.\")\n",
    "    log_mensagem(log_path, \"final\", \"Processamento concluído\")\n",
    "\n",
    "finally:\n",
    "    keep_alive_flag[\"running\"] = False\n",
    "    keep_alive_thread.join(timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f06f86",
   "metadata": {},
   "source": [
    "#### **Request last toponyms and metadata – Overpass API**\n",
    "\n",
    " * [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54578457",
   "metadata": {},
   "source": [
    "##### Implementação sem módulos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7: Último topônimo OSM por classe (via Overpass API) ===\n",
    "# Sem módulo utils\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time, threading, math, json, csv, glob\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIGURAÇÕES GERAIS ===\n",
    "output_dir = Path(\"data/output_code1/step7_latest_name_overpass\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = output_dir / \"log_step7_overpass.csv\"\n",
    "ultimo_lote_path = output_dir / \"ultimo_lote_step7_overpass.txt\"\n",
    "\n",
    "# === INICIALIZAÇÃO DO LOG ===\n",
    "if not log_path.exists():\n",
    "    with open(log_path, 'w', newline='') as f:\n",
    "        csv.writer(f).writerow([\"lote\", \"mensagem\", \"timestamp\"])\n",
    "\n",
    "def log_mensagem(lote, mensagem):\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(log_path, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([lote, mensagem, timestamp])\n",
    "\n",
    "# === KEEP ALIVE (com controle de término) ===\n",
    "keep_alive_running = True\n",
    "def keep_alive():\n",
    "    while keep_alive_running:\n",
    "        time.sleep(300)\n",
    "        print(\"Ainda trabalhando...\")\n",
    "        log_mensagem(\"keep_alive\", \"Ainda trabalhando...\")\n",
    "\n",
    "keep_alive_thread = threading.Thread(target=keep_alive, daemon=True)\n",
    "keep_alive_thread.start()\n",
    "\n",
    "# === EXECUÇÃO EM LOTE USANDO OVERPASS ===\n",
    "try:\n",
    "    ultimo_lote = 0\n",
    "    if ultimo_lote_path.exists():\n",
    "        with open(ultimo_lote_path, 'r') as f:\n",
    "            ultimo_lote = int(f.read().strip())\n",
    "\n",
    "    lote_size = 20\n",
    "    total_lotes = math.ceil(len(gdf_cells_valid) / lote_size)\n",
    "\n",
    "    for lote_index in range(ultimo_lote, total_lotes):\n",
    "        start_time = time.time()\n",
    "        features_final = []\n",
    "\n",
    "        subset = gdf_cells_valid.iloc[lote_index * lote_size: (lote_index + 1) * lote_size]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(processar_com_overpass, row, classe_et_edgv_to_tags, log_mensagem) for _, row in subset.iterrows()]\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    features_final.extend(future.result())\n",
    "                except Exception as e:\n",
    "                    log_mensagem(lote_index + 1, f\"[FALHA GERAL]: {e}\")\n",
    "\n",
    "        # Salva lote em GeoJSON\n",
    "        fc = {\"type\": \"FeatureCollection\", \"features\": features_final}\n",
    "        out_path = output_dir / f\"step7_lote{lote_index + 1}.geojson\"\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(fc, f)\n",
    "        log_mensagem(lote_index + 1, f\"SALVO {out_path.name}\")\n",
    "\n",
    "        # Atualiza consolidação incremental\n",
    "        arquivos = sorted(output_dir.glob(\"step7_lote*.geojson\"))\n",
    "        todas_features = []\n",
    "        for arquivo in arquivos:\n",
    "            with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "                fc_parcial = json.load(f)\n",
    "                todas_features.extend(fc_parcial['features'])\n",
    "\n",
    "        final_fc = {\"type\": \"FeatureCollection\", \"features\": todas_features}\n",
    "        with open(output_dir / \"step7_consolidado.geojson\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_fc, f)\n",
    "        log_mensagem(lote_index + 1, \"CONSOLIDADO atualizado\")\n",
    "\n",
    "        with open(ultimo_lote_path, 'w') as f:\n",
    "            f.write(str(lote_index + 1))\n",
    "\n",
    "        tempo_msg = f\"Tempo lote {lote_index + 1}: {str(timedelta(seconds=int(time.time() - start_time)))}\"\n",
    "        print(tempo_msg)\n",
    "        log_mensagem(lote_index + 1, tempo_msg)\n",
    "\n",
    "    print(\"Step 7 (último topônimo por classe - Overpass) finalizado com sucesso.\")\n",
    "    log_mensagem(\"step7_overpass\", \"Processamento finalizado\")\n",
    "\n",
    "finally:\n",
    "    keep_alive_running = False\n",
    "    keep_alive_thread.join(timeout=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93247bc3",
   "metadata": {},
   "source": [
    "##### Implementação com módulos externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c48a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7 (Overpass API) ===\n",
    "# Com módulos utils e processar_com_overpass\n",
    "\n",
    "from src.utils import init_log, start_keep_alive, log_mensagem, consolidar_geojson\n",
    "from src.processar_com_overpass import processar_com_overpass\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import math, json, time\n",
    "\n",
    "# === CONFIGURAÇÃO ===\n",
    "output_dir = Path(\"data/output_code1/step7_latest_name_overpass\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_path = output_dir / \"log_step7_overpass.csv\"\n",
    "ultimo_lote_path = output_dir / \"ultimo_lote_step7_overpass.txt\"\n",
    "\n",
    "# === LOG E KEEP ALIVE ===\n",
    "init_log(log_path)\n",
    "keep_alive_flag = {\"running\": True}\n",
    "keep_alive_thread = start_keep_alive(log_path, keep_alive_flag)\n",
    "\n",
    "# === EXECUÇÃO EM LOTE ===\n",
    "try:\n",
    "    ultimo_lote = int(ultimo_lote_path.read_text().strip()) if ultimo_lote_path.exists() else 0\n",
    "    lote_size = 20\n",
    "    total_lotes = math.ceil(len(gdf_cells_valid) / lote_size)\n",
    "\n",
    "    for lote_index in range(ultimo_lote, total_lotes):\n",
    "        start_time = time.time()\n",
    "        features_final = []\n",
    "        subset = gdf_cells_valid.iloc[lote_index * lote_size: (lote_index + 1) * lote_size]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [\n",
    "                executor.submit(processar_com_overpass, row, classe_et_edgv_to_tags, log_mensagem, log_path)\n",
    "                for _, row in subset.iterrows()\n",
    "            ]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Lote {lote_index + 1} (Step 7 Overpass)\"):\n",
    "                try:\n",
    "                    features_final.extend(future.result())\n",
    "                except Exception as e:\n",
    "                    log_mensagem(log_path, lote_index + 1, f\"[FALHA GERAL]: {e}\")\n",
    "\n",
    "        # Salva lote\n",
    "        out_path = output_dir / f\"step7_lote{lote_index + 1}.geojson\"\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"type\": \"FeatureCollection\", \"features\": features_final}, f)\n",
    "        log_mensagem(log_path, lote_index + 1, f\"SALVO {out_path.name}\")\n",
    "\n",
    "        # Consolida incremental\n",
    "        total_feats = consolidar_geojson(output_dir, \"step7_lote*.geojson\", \"step7_consolidado.geojson\")\n",
    "        log_mensagem(log_path, lote_index + 1, f\"CONSOLIDADO atualizado: {total_feats} features\")\n",
    "\n",
    "        # Atualiza lote\n",
    "        ultimo_lote_path.write_text(str(lote_index + 1))\n",
    "        print(f\"Lote {lote_index + 1} concluído em {str(timedelta(seconds=int(time.time() - start_time)))}\")\n",
    "\n",
    "    print(\"Step 7 (Overpass) finalizado com sucesso.\")\n",
    "    log_mensagem(log_path, \"final\", \"Processamento concluído\")\n",
    "\n",
    "finally:\n",
    "    keep_alive_flag[\"running\"] = False\n",
    "    keep_alive_thread.join(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG MANUAL - PROCESSAR UMA CELULA ===\n",
    "\n",
    "# Rodar debug manual com apenas uma célula\n",
    "test_row = gdf_cells_valid.iloc[0]\n",
    "features = processar_com_overpass.processar_com_overpass(test_row, classe_et_edgv_to_tags, log_mensagem, log_path)\n",
    "\n",
    "print(json.dumps(features, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938b8f7",
   "metadata": {},
   "source": [
    "#### PostGIS - Close the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11755da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "if conn and conn.closed == 0:\n",
    "    # conexão ainda aberta\n",
    "    with conn.cursor() as cur:\n",
    "        ...\n",
    "        cur.close()\n",
    "    conn.close()\n",
    "    print(\"Conexão com o banco de dados fechada.\")\n",
    "else:\n",
    "    print(\"Conexão com o banco de dados já estava fechada ou não foi estabelecida.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87d3cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-DScPythonGeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
